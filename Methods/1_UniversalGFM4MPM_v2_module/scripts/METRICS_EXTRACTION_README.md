# Meta-Evaluation & Metrics System

## Overview
The meta-evaluation system provides a comprehensive framework for evaluating Positive-Unlabeled (PU) classifiers, particularly in the context of geological modeling where true negatives are often unknown. It goes beyond simple accuracy by using **Positive Dropout (PosDrop)**, **Target Shuffling**, and **Stability Selection** to assess model robustness and significance.

## Rigorous Evaluation Framework

To distinguish true geological signals from modeling artifacts and evaluate performance without reliable negatives, we employ a suite of rigorous checks:

### 1. Positive Dropout (Sensitivity Check)
Since we lack reliable negative labels, we evaluate performance by "hiding" a subset of known positive samples during training and measuring the model's ability to "rediscover" them.
- **Mechanism**: Iteratively drops a subset of positive labels (treating them as unlabeled) and trains the model.
- **Metric**: `accuracy` (mean prediction score of dropped positives).

### 2. Target Shuffling (Bias Check)
Quantifies the "optimism" of the metric by comparing real performance against a null distribution generated by shuffling labels.
- **`z_score`**: Number of standard deviations the real metric is above the mean of the shuffled metrics. High Z-Score (>3) indicates the result is unlikely due to chance.
- **`fde` (False Discovery Excess)**: The excess false positive rate at a fixed TPR (e.g., 90%) compared to random chance. `FDE = Real_FPR - Mean_Shuffle_FPR`. Lower is better.
- **`background_rejection`**: Fraction of background samples correctly rejected at a high TPR (e.g., 90%).

### 3. Stability Selection (Variance Check)
Assesses the robustness of the selected features/predictions by training on random subsets of the data.
- **`spatial_entropy`**: Measures the uncertainty of the spatial distribution of predictions. Lower entropy implies more consistent predictions across subsets.
- **`spatial_jaccard`**: Jaccard index measuring the overlap of top-k predictions between different subsets. Higher Jaccard implies greater stability.
- **`stability_scalars`**: Mean and Standard Deviation of scalar metrics (Accuracy, Lift, PAUC) across subsets. High variance indicates instability.

### 4. Baseline Check
- **`lift`**: The ratio of the capture rate in the top-K% of predictions to the expected capture rate if predictions were random. `Lift = Capture_Rate / Top_K_Percent`. Values > 1 indicate better-than-random performance.

## Configuration (`MetaEvaluationConfig`)

The evaluation is controlled by `MetaEvaluationConfig` (in `train_cls_PN_base.py`) and command-line arguments:

| Argument | Description | Default |
|----------|-------------|---------|
| `--meta-eval-metrics` | Set of metrics to compute | `accuracy`, `Focus`, `pauc`, `topk`, `lift` |
| `--meta-eval-posdrop-clusters` | Number of clusters for PosDrop iteration | `[10]` |
| `--meta-eval-posdrop-strategy` | Strategy for selecting negatives | `['latent_dist']` |
| `--meta-eval-shuffle-runs` | Number of runs for Target Shuffling | `0` (disabled) |
| `--meta-eval-stability-runs` | Number of runs for Stability Selection | `0` (disabled) |
| `--meta-eval-stability-rate` | Subsampling rate for Stability Selection | `0.8` |

## Workflow

1.  **Training**: `train.py` calls `train_cls_1_PN_PosDrop_MultiClustering`.
2.  **PosDrop Loop**: Iterates through folds, training on a subset of positives.
3.  **Inference**: Computes predictions for the entire dataset (or specific subsets).
4.  **Metric Computation**:
    *   `compute_accuracy`: Basic PosDrop accuracy.
    *   `compute_extended_meta_evaluation`: Computes PAUC, TopK, Lift, and PU-specific metrics.
5.  **Rigorous Checks** (if enabled):
    *   **Target Shuffling**: Repeats training with shuffled labels to build a null distribution.
    *   **Stability Selection**: Repeats training with subsampled data to measure variance.
6.  **Aggregation**: Results are aggregated across folds and configurations.
7.  **Saving**: Results are saved to `{tag}_meta_eval.json`.

## Output Structure (`{tag}_meta_eval.json`)

```json
{
  "accuracy": {
    "mean": 0.85,
    "std": 0.02,
    "scores": [...]
  },
  "pauc": {
    "0.01": { "mean": 0.78, "std": 0.01, ... },
    ...
  },
  "topk": {
    "area_percentages": {
      "1.0": { "mean_lift": 5.2, ... }
    }
  },
  "rigorous_evaluation": {
    "target_shuffling": {
      "z_score": 4.5,
      "fde": -0.12,
      ...
    },
    "stability_selection": {
      "spatial_entropy": 0.45,
      "spatial_jaccard": 0.72,
      ...
    }
  }
}
```

## Usage Example

```bash
# Basic training with PosDrop
python -m Methods.1_UniversalGFM4MPM_v2_module.scripts.train ...

# Enable Rigorous Evaluation (Shuffling & Stability)
python -m Methods.1_UniversalGFM4MPM_v2_module.scripts.train ... \
    --meta-eval-shuffle-runs 5 \
    --meta-eval-stability-runs 5
```
